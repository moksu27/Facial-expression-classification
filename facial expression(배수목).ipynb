{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import glob\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session() # 새로운 세션으로 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 128\n",
    "width = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_read_resize(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (height, width))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_folder_read(root_dir):\n",
    "    img_files = []\n",
    "    nfiles = glob.glob(f\"{root_dir}/*\")\n",
    "    nfiles = sorted(nfiles)\n",
    "    for n_img in nfiles:\n",
    "        try:\n",
    "            img_files.append(img_read_resize(n_img))\n",
    "        except:\n",
    "            continue\n",
    "    return img_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archive/test'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_root_dir = \"archive/test\"\n",
    "test_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(rescale = 1./255, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    directory = \"archive/train\",\n",
    "    color_mode = 'grayscale',\n",
    "    target_size = (height,width),\n",
    "    batch_size = 64,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = data_generator.flow_from_directory(\n",
    "    directory = \"archive/train\",\n",
    "    color_mode = 'grayscale',\n",
    "    target_size =(height, width),\n",
    "    class_mode = 'categorical',\n",
    "    batch_size = 64,\n",
    "    subset='validation',\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 입력층\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', input_shape=(height, width, 1)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Fully-connected layer\n",
    "\n",
    "# 출력층\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# vgg 사용\\nfrom tensorflow.keras.applications.vgg16 import VGG16\\nvgg = VGG16(weights = \"imagenet\", include_top=False, input_shape =(height, width, 3))\\n\\nmodel = Sequential()\\nmodel.add(vgg)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024, activation = \\'relu\\'))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(7,activation = \"softmax\"))\\n'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# vgg 사용\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "vgg = VGG16(weights = \"imagenet\", include_top=False, input_shape =(height, width, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(7,activation = \"softmax\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 64)      640       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 126, 126, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 126, 126, 128)    512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 63, 63, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 63, 63, 64)        73792     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 61, 61, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 61, 61, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 30, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 115200)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              117965824 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,245,767\n",
      "Trainable params: 119,245,255\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "359/359 [==============================] - 1507s 4s/step - loss: 12.5438 - accuracy: 0.2137 - val_loss: 8.3573 - val_accuracy: 0.2376\n",
      "Epoch 2/2\n",
      "359/359 [==============================] - 1530s 4s/step - loss: 1.9229 - accuracy: 0.2655 - val_loss: 2.3908 - val_accuracy: 0.2662\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, epochs = 2, callbacks = early_stop, validation_data = val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a94d6498c08481623d29bad213a6943bcc71eecdd7acbc55fe0d7d47d1049f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
